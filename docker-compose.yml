services:
  # Ollama Service (local models)
  ollama:
    image: ollama/ollama:latest
    # Do NOT pin platform; image is multi-arch (AMD64/ARM64)
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_PARALLEL=1
    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
    restart: unless-stopped
    networks:
      - app-network

  # Model Puller Service
  model-puller:
    image: alpine:3.20
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      OLLAMA_HOST: http://ollama:11434
      MODELS_CONFIG: /config/models.yaml
      ENABLE_AUTO_PULL: "true"
      PULL_RETRIES: "5"
      PULL_SLEEP_SECS: "3"
    volumes:
      - ./config:/config:ro
      - ./scripts/pull-models.sh:/pull-models.sh:ro
    # Make sure script runs even if Windows checked out with CRLF
    entrypoint: ["/bin/sh", "-lc", "apk add --no-cache dos2unix >/dev/null 2>&1 || true; dos2unix /pull-models.sh >/dev/null 2>&1 || true; chmod +x /pull-models.sh; /pull-models.sh"]
    restart: "no"
    networks:
      - app-network

  # Backend API Service
  api:
    build: ./backend
    environment:
      MODELS_CONFIG: /config/models.yaml
      MODELS_CONFIG_PATH: /config/models.yaml
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      HOST: 0.0.0.0
      PORT: 8080
      # Chat model API keys
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      CEREBRAS_API_KEY: ${CEREBRAS_API_KEY}
      # Embedding model API keys
      COHERE_API_KEY: ${COHERE_API_KEY}
      VOYAGE_API_KEY: ${VOYAGE_API_KEY}
      # Ollama connection
      OLLAMA_HOST: http://ollama:11434
      LANGSMITH_TRACING: ${LANGSMITH_TRACING}
      LANGSMITH_API_KEY: ${LANGSMITH_API_KEY}
    volumes:
      - ./config:/config:ro
      - ./backend/logs:/app/logs
    ports:
      - "8080:8080"
    depends_on:
      model-puller:
        condition: service_completed_successfully
    # Ensure curl exists in the image OR change to wget if needed
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - app-network

  # Frontend UI Service
  ui:
    build: ./ui
    environment:
      NEXT_PUBLIC_API_BASE_URL: http://localhost:8080
      NEXT_PUBLIC_API_URL: http://api:8080
      NODE_ENV: production
    ports:
      - "3000:3000"
    depends_on:
      - api
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  ollama:
    driver: local